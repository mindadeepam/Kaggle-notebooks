{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/mindadeepam/classification-algorithms?scriptVersionId=89120031\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"!pip install openpyxl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\n\ndf = pd.read_excel(\"../input/pumpkin-seeds-dataset/Pumpkin_Seeds_Dataset.xlsx\")\n# test = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes           # 12 numeric features - 1 object class label\ndf.Class.unique()   # 2 unique classes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()           # no null values in df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"sns.countplot(x='Class',data=df)      ## over 1000 samples for both","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(data=df,  hue='Class')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Equiv_Diameter, Convex_Area and Area seem highly correalated\ntemp_data = df[['Equiv_Diameter','Convex_Area', 'Area', 'Class']]\nsns.pairplot(temp_data, hue=\"Class\")\nprint(temp_data.corr())                         ## >0.99 correlation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.subplots(2,2)\nsns.pairplot(temp_data[temp_data['Class']=='Çerçevelik'], palette='tab10')\nsns.pairplot(temp_data[temp_data['Class']!='Çerçevelik'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Compactness, Aspect_Ration and Eccentricity seem highly correlated\ntemp_data = df[['Compactness', 'Aspect_Ration', 'Eccentricity','Class']]\nsns.pairplot(temp_data, hue='Class')\nprint(temp_data.corr())\n\n# Compactness and Aspect_ratio have -.99 corr, \n# Comp and Eccen have -.98 corr\n# Eccen and Aspect have .95 corr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### But as seen while building models, dropping any single or multiple collinear features doesnt improve performance ","metadata":{}},{"cell_type":"markdown","source":"## Remove Outliers ..","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x='Eccentricity',data=df)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Models","metadata":{}},{"cell_type":"code","source":"## does dropping collinear features increase accuracy??\n\ndf_ = df.drop(['Aspect_Ration'], axis=1)      ## harms performance\n# 'Compactness', 'Aspect_Ration', 'Eccentricity' ; 'Convex_Area??', 'Area', Equiv_Diameter'\n# remove =   nothing seems to give better preformance consistently\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_ = df.drop(['Equiv_Diameter'], axis=1)        ## dropping doesnt seem to have any benefits\nX = df.drop('Class',axis=1).to_numpy()\ny = df[\"Class\"].to_numpy()\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, stratify=y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n# why does scaling harm performance in l2 ..??\n# but scaling required for liblinear solver","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"code","source":"# clf = LogisticRegression( C=1, random_state=460, max_iter=300)\n\nclf = LogisticRegression(solver='liblinear', penalty='l1', C=1000, random_state=460, max_iter=500)\n## as we make reg constant very large, ie C-->0 == score--> 0.52] ;  l2\n## as c->> very large, ie as regularization->0 score->.88 ; .87\n\nclf.fit(X_train, y_train)\nprint(clf.score(X_train, y_train), clf.score(X_test, y_test))      \n# train 0.88","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- l2 reg score = 0.875, .88 without scaling\n- l1 reg score = 0.875 0.9056","metadata":{}},{"cell_type":"markdown","source":"## KNN","metadata":{}},{"cell_type":"code","source":"clf = KNeighborsClassifier(n_neighbors=20).fit(X_train, y_train)\nprint(clf.score(X_train, y_train), clf.score(X_test, y_test))   \n## train 0.89","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- .889 train ; .875 test \n- not much different from logistic regression","metadata":{}},{"cell_type":"code","source":"## encoding of labels is not required for sklearn classifiers\ndef encode(x):\n    if x == 'Çerçevelik':\n        return 0\n    return 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SVM","metadata":{}},{"cell_type":"code","source":"clf = svm.SVC(kernel='poly', degree=1)\nclf.fit(X_train, y_train)\nprint(clf.score(X_train, y_train), clf.score(X_test, y_test)) \n  \n## poly 1-.873, .893 ; 2-.78, .766 ; 3-.85, .85; 4-.77, .77; 5-.80, .79...  drops off as degree increases\n## rbf = .88, .88","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### why is svm giving low score sometimes.....??? also, increasing degree in poly kernel doesnt increase train accuracy.??\n","metadata":{}},{"cell_type":"markdown","source":"## Naive Bayes","metadata":{}},{"cell_type":"code","source":"## though not all features have gaussian distribution, that would be the best assumption..\n","metadata":{},"execution_count":null,"outputs":[]}]}